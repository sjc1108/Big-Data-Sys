{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326874e2-edd0-4250-a27e-697362d96556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address        Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.144.3  121.45 KiB  16      100.0%            67dca57f-7b20-4fca-9d27-4236d876c191  rack1\n",
      "UN  192.168.144.4  121.42 KiB  16      100.0%            9aa0ef31-b9ee-42ed-9517-808e0336a12f  rack1\n",
      "UN  192.168.144.2  126.51 KiB  16      100.0%            5c3b3a31-6a8a-46fe-a32c-e6383b30ada6  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5162f45-da62-4a37-801e-cb9f8d0583d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from subprocess import check_output\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import grpc\n",
    "import station_pb2\n",
    "import station_pb2_grpc\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86379fac-f836-4b26-bcf5-8d9f6a8f7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfaa86c0-79ce-4edd-9f49-715f18c0af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database setup complete.\n"
     ]
    }
   ],
   "source": [
    "cass.execute(\"DROP KEYSPACE IF EXISTS weather;\")\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE KEYSPACE weather\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};\n",
    "\"\"\")\n",
    "cass.set_keyspace('weather')\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TYPE station_record (\n",
    "        tmin int,\n",
    "        tmax int\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "cass.execute(\"\"\"\n",
    "    CREATE TABLE stations (\n",
    "        id text,\n",
    "        name text STATIC,\n",
    "        date date,\n",
    "        record station_record,\n",
    "        PRIMARY KEY (id, date)\n",
    "    ) WITH CLUSTERING ORDER BY (date ASC);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"Database setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388584c0-287a-4033-99cf-a308bfef57aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "schema = cass.execute(\"DESCRIBE TABLE weather.stations\")\n",
    "schema.one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3221c25-aed4-4ec1-b2b4-519dd5a7e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fa60381c-154f-4146-b0ed-70b1f8eb3e7f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 1255ms :: artifacts dl 79ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fa60381c-154f-4146-b0ed-70b1f8eb3e7f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/30ms)\n",
      "24/04/18 22:58:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0d38a9-6c36-4aeb-a529-d5e81a6fb451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"ghcnd-stations.txt\"\n",
    "stations_df = spark.read.text(path)\n",
    "processed_stations = stations_df.select(\n",
    "    expr(\"substring(value, 1, 11)\").alias(\"id\"),\n",
    "    expr(\"substring(value, 39, 2)\").alias(\"state\"),\n",
    "    expr(\"trim(substring(value, 42, 30))\").alias(\"name\")\n",
    ")\n",
    "\n",
    "# filter WI\n",
    "stations_wi = processed_stations.filter(processed_stations.state == \"WI\")\n",
    "cass.execute(\"DROP TABLE IF EXISTS weather.stations;\")\n",
    "cass.execute(\"\"\"\n",
    "create table weather.stations(\n",
    "    id text, \n",
    "    name text static,\n",
    "    date date,\n",
    "    record station_record,\n",
    "    PRIMARY KEY(id, date)   \n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "for row in stations_wi.collect():\n",
    "    cass.execute(\n",
    "        \"INSERT INTO weather.stations (id, name) VALUES (%s, %s)\",\n",
    "        (row.id, row.name)\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4629ccc-f4db-45e4-b9cc-4f0d7e398ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMBERG 1.3 SW'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "result = cass.execute(\"\"\" \n",
    "    SELECT name \n",
    "    FROM weather.stations \n",
    "    WHERE id = 'US1WIMR0003'\n",
    "    \"\"\"\n",
    ")\n",
    "q2 = result.one()[0]\n",
    "q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba70e95-6b2a-4d8e-a077-3ff6b3359429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "result_q3 = cass.execute(\"\"\"\n",
    "    SELECT token(id) \n",
    "    FROM weather.stations \n",
    "    WHERE id = 'USC00470273'\n",
    "    \"\"\")\n",
    "q3=result_q3.one()[0]\n",
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a398f879-aa1d-49b8-a984-b48852030a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8849592649763465485\n"
     ]
    }
   ],
   "source": [
    "#q4\n",
    "output = check_output([\"nodetool\", \"ring\"])\n",
    "\n",
    "tokens = []\n",
    "my_output = output.decode().split(\" \")\n",
    "tokens = [l for l in my_output if l[1:].isnumeric()]\n",
    "        \n",
    "max = int(tokens[0])\n",
    "min = int(tokens[1])\n",
    "tokens = tokens[1:]\n",
    "for i in range(len(tokens)):\n",
    "    token = int(tokens[i])\n",
    "    \n",
    "    if q3 < min or q3 > max:\n",
    "        print(min)\n",
    "        break\n",
    "        \n",
    "    if q3 < int(token):\n",
    "        print(tokens[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf8d958-9a94-4035-9635-37df2fdd7220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  records.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -n records.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff9c868-2c53-4936-9365-66cfb5cf5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TMAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USW00014898</td>\n",
       "      <td>20220107</td>\n",
       "      <td>-166.0</td>\n",
       "      <td>-71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USW00014839</td>\n",
       "      <td>20220924</td>\n",
       "      <td>117.0</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USW00014839</td>\n",
       "      <td>20220523</td>\n",
       "      <td>83.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USW00014839</td>\n",
       "      <td>20221019</td>\n",
       "      <td>11.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USW00014839</td>\n",
       "      <td>20220529</td>\n",
       "      <td>139.0</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>USW00014898</td>\n",
       "      <td>20220724</td>\n",
       "      <td>167.0</td>\n",
       "      <td>278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>USW00014837</td>\n",
       "      <td>20221004</td>\n",
       "      <td>50.0</td>\n",
       "      <td>222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>USW00014837</td>\n",
       "      <td>20221107</td>\n",
       "      <td>17.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>USW00014898</td>\n",
       "      <td>20221006</td>\n",
       "      <td>56.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>USR0000WDDG</td>\n",
       "      <td>20221021</td>\n",
       "      <td>72.0</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          station      date   TMIN   TMAX\n",
       "0     USW00014898  20220107 -166.0  -71.0\n",
       "1     USW00014839  20220924  117.0  194.0\n",
       "2     USW00014839  20220523   83.0  150.0\n",
       "3     USW00014839  20221019   11.0   83.0\n",
       "4     USW00014839  20220529  139.0  261.0\n",
       "...           ...       ...    ...    ...\n",
       "1455  USW00014898  20220724  167.0  278.0\n",
       "1456  USW00014837  20221004   50.0  222.0\n",
       "1457  USW00014837  20221107   17.0   94.0\n",
       "1458  USW00014898  20221006   56.0  200.0\n",
       "1459  USR0000WDDG  20221021   72.0  239.0\n",
       "\n",
       "[1460 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = spark.read.parquet(\"records.parquet\")\n",
    "records = records.groupBy(\"station\",\"date\").pivot(\"element\",[\"TMIN\",\"TMAX\"])\n",
    "records.sum().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a83cc75-8825-4b7d-823e-688a9b3fc023",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel(\"localhost:5440\")\n",
    "channel_stub = station_pb2_grpc.StationStub(channel)\n",
    "collection = records.sum().collect()\n",
    "\n",
    "for i in collection:\n",
    "    station = i[\"station\"]\n",
    "    date = i[\"date\"]\n",
    "    tmin = i[\"TMIN\"]\n",
    "    tmax = i[\"TMAX\"]\n",
    "  \n",
    "    date = datetime.strptime(date, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    response = channel_stub.RecordTemps(station_pb2.RecordTempsRequest(station=station, date=date, tmin=int(tmin), tmax=int(tmax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d6bb537-746c-44d2-a306-c3a8486e1972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "q5 = (channel_stub.StationMax(station_pb2.StationMaxRequest(station = \"USW00014837\"))).tmax\n",
    "q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c4a0ca2-a317-48e6-9504-3027462019e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\") \\\n",
    "    .option(\"keyspace\", \"weather\") \\\n",
    "    .option(\"table\", \"stations\") \\\n",
    "    .load() \\\n",
    "    .createOrReplaceTempView(\"stations\")\n",
    "\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3315e009-e6c2-40c5-bdbb-7154f92ebd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/18 22:59:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'USW00014839': 89.6986301369863,\n",
       " 'USW00014837': 105.62739726027397,\n",
       " 'USW00014898': 102.93698630136986,\n",
       " 'USR0000WDDG': 102.06849315068493}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TemperatureDifference\") \\\n",
    "    .getOrCreate()\n",
    "stations_sql = spark.sql(\"SELECT * FROM stations\")\n",
    "\n",
    "temp_diff = stations_sql.withColumn(\"temperature_diff\", col(\"record.tmax\") - col(\"record.tmin\"))\n",
    "\n",
    "filtered = temp_diff.filter(col(\"record\").isNotNull())\n",
    "\n",
    "avg_diff = filtered.groupBy(\"id\").avg(\"temperature_diff\")\n",
    "avg_diff_dict_q7 = {row['id']: row['avg(temperature_diff)'] for row in avg_diff.collect()}\n",
    "\n",
    "avg_diff_dict_q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2282354-017d-47d0-aa50-5d0c0bb5580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address        Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.144.3  227.73 KiB  16      100.0%            67dca57f-7b20-4fca-9d27-4236d876c191  rack1\n",
      "UN  192.168.144.4  226.99 KiB  16      100.0%            9aa0ef31-b9ee-42ed-9517-808e0336a12f  rack1\n",
      "UN  192.168.144.2  233.54 KiB  16      100.0%            5c3b3a31-6a8a-46fe-a32c-e6383b30ada6  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e34543cf-691d-4d84-9f99-ce0a6af49fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "(channel_stub.StationMax(station_pb2.StationMaxRequest(station = \"USW00014837\"))).error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60777617-5c52-4482-a4be-a17b8901e5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "(channel_stub.RecordTemps(station_pb2.RecordTempsRequest(station=\"USW00014837\", date=\"2023-03-12\", tmin=10, tmax=90))).error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
